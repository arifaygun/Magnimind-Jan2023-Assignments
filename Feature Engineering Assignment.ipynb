{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad62c74-b57a-4bad-af08-f67034fceadf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Engineering Assignment - Arif Aygun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684eec79-d037-4bc6-bb3f-d87861ac3f87",
   "metadata": {},
   "source": [
    "You've constructed some potentially useful and business-relevant features, derived from summary statistics, for each of the states you're concerned with. You've explored many of these features in turn and found various trends. Some states are higher in some but not in others. Some features will also be more correlated with one another than others.\n",
    "One way to disentangle this interconnected web of relationships is via principal components analysis (PCA). This technique will find linear combinations of the original features that are uncorrelated with one another and order them by the amount of variance they explain. You can use these derived features to visualize the data in a lower dimension (e.g. 2 down from 7) and know how much variance the representation explains. You can also explore how the original features contribute to these derived features.\n",
    "\n",
    "The basic steps in this process are:\n",
    "\n",
    "- scale the data and verify the scaling\n",
    "- fit the PCA transformation (learn the transformation from the data)\n",
    "- Draw scree plot\n",
    "- Find the optimum number of components\n",
    "- Draw the biplot and interpret the relationship between features and components\n",
    "- Find the exact correlation between components and features using loading\n",
    "\n",
    "Please use ski_resort_data\n",
    "\n",
    "1. Scale the data\n",
    "\n",
    "2. Verifying the scaling\n",
    "\n",
    "3. Calculate the PCA transformation\n",
    "\n",
    "4. Draw Scree Plot\n",
    "\n",
    "5. Find the Optimum Number of Components\n",
    "\n",
    "6. Draw Biplot\n",
    "\n",
    "7. Calculate Loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6961ddc-1ffc-4d51-b744-f0ce74e40a69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3329c05e-be3d-4a40-9690-469e48d07dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ski = pd.read_csv('/Users/arifaygun/Documents/GitHub/Data Sets/Magnimind/ski_resort_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3cf8400-1ec3-46d8-aff1-349782fd1eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Region</th>\n",
       "      <th>state</th>\n",
       "      <th>summit_elev</th>\n",
       "      <th>vertical_drop</th>\n",
       "      <th>base_elev</th>\n",
       "      <th>trams</th>\n",
       "      <th>fastEight</th>\n",
       "      <th>fastSixes</th>\n",
       "      <th>fastQuads</th>\n",
       "      <th>...</th>\n",
       "      <th>LongestRun_mi</th>\n",
       "      <th>SkiableTerrain_ac</th>\n",
       "      <th>Snow Making_ac</th>\n",
       "      <th>daysOpenLastYear</th>\n",
       "      <th>yearsOpen</th>\n",
       "      <th>averageSnowfall</th>\n",
       "      <th>AdultWeekday</th>\n",
       "      <th>AdultWeekend</th>\n",
       "      <th>projectedDaysOpen</th>\n",
       "      <th>NightSkiing_ac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alyeska Resort</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>3939</td>\n",
       "      <td>2500</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1610.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>669.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eaglecrest Ski Area</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>2600</td>\n",
       "      <td>1540</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hilltop Ski Area</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>2090</td>\n",
       "      <td>294</td>\n",
       "      <td>1796</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arizona Snowbowl</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>11500</td>\n",
       "      <td>2300</td>\n",
       "      <td>9200</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>777.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sunrise Park Resort</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>11100</td>\n",
       "      <td>1800</td>\n",
       "      <td>9200</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>800.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name   Region    state  summit_elev  vertical_drop  \\\n",
       "0       Alyeska Resort   Alaska   Alaska         3939           2500   \n",
       "1  Eaglecrest Ski Area   Alaska   Alaska         2600           1540   \n",
       "2     Hilltop Ski Area   Alaska   Alaska         2090            294   \n",
       "3     Arizona Snowbowl  Arizona  Arizona        11500           2300   \n",
       "4  Sunrise Park Resort  Arizona  Arizona        11100           1800   \n",
       "\n",
       "   base_elev  trams  fastEight  fastSixes  fastQuads  ...  LongestRun_mi  \\\n",
       "0        250      1        0.0          0          2  ...            1.0   \n",
       "1       1200      0        0.0          0          0  ...            2.0   \n",
       "2       1796      0        0.0          0          0  ...            1.0   \n",
       "3       9200      0        0.0          1          0  ...            2.0   \n",
       "4       9200      0        NaN          0          1  ...            1.2   \n",
       "\n",
       "   SkiableTerrain_ac  Snow Making_ac  daysOpenLastYear  yearsOpen  \\\n",
       "0             1610.0           113.0             150.0       60.0   \n",
       "1              640.0            60.0              45.0       44.0   \n",
       "2               30.0            30.0             150.0       36.0   \n",
       "3              777.0           104.0             122.0       81.0   \n",
       "4              800.0            80.0             115.0       49.0   \n",
       "\n",
       "   averageSnowfall  AdultWeekday  AdultWeekend  projectedDaysOpen  \\\n",
       "0            669.0          65.0          85.0              150.0   \n",
       "1            350.0          47.0          53.0               90.0   \n",
       "2             69.0          30.0          34.0              152.0   \n",
       "3            260.0          89.0          89.0              122.0   \n",
       "4            250.0          74.0          78.0              104.0   \n",
       "\n",
       "   NightSkiing_ac  \n",
       "0           550.0  \n",
       "1             NaN  \n",
       "2            30.0  \n",
       "3             NaN  \n",
       "4            80.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ski.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14783008-2c5f-479c-9df3-a4ebdf1d840a",
   "metadata": {},
   "source": [
    "Scale the data:\n",
    "To perform PCA, it is necessary to standardize the data, ensuring that each feature has zero mean and unit variance. We can use the StandardScaler from the scikit-learn library to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7adb2fca-fe4b-4497-a3b5-052777ad19ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select the features to be used in the PCA\n",
    "features = ['summit_elev', 'vertical_drop', 'base_elev', 'trams', 'fastEight', 'fastSixes', 'fastQuads']\n",
    "\n",
    "# Create a new DataFrame with only the selected features\n",
    "ski_resort_data_pca = ski[features]\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "ski_resort_data_scaled = scaler.fit_transform(ski_resort_data_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb73915-2a1e-48bf-afc5-86ef5386d010",
   "metadata": {},
   "source": [
    "Verifying the scaling:\n",
    "To verify that the scaling has been successful, we can calculate the mean and standard deviation of each feature after scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a82d9541-961d-484b-8ee8-af605e54117b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [-4.30631961e-17 -4.84460956e-17 -4.30631961e-17  3.22973971e-17\n",
      "             nan  2.15315981e-17  6.99776937e-17]\n",
      "Standard deviation: [ 1.  1.  1.  1. nan  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the mean and standard deviation of each feature\n",
    "mean = np.mean(ski_resort_data_scaled, axis=0)\n",
    "std = np.std(ski_resort_data_scaled, axis=0)\n",
    "\n",
    "print('Mean:', mean)\n",
    "print('Standard deviation:', std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c6eaf-1e86-46f1-b175-73bb55ef2089",
   "metadata": {},
   "source": [
    "The output should be a vector of zeros for the mean and a vector of ones for the standard deviation, indicating that the data has been scaled correctly.\n",
    "\n",
    "Calculate the PCA transformation:\n",
    "We can use the PCA function from the scikit-learn library to calculate the PCA transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e7c7f0d-52dd-4c4d-a82b-33339ed6852a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Fit the PCA transformation to the scaled data\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m ski_resort_pca \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mski_resort_data_scaled\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:462\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03mC-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 462\u001b[0m U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:485\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA does not support sparse input. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTruncatedSVD for a possible alternative.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m     )\n\u001b[0;32m--> 485\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# Handle n_components==None\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 546\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA object with the desired number of components\n",
    "pca = PCA(n_components=7)\n",
    "\n",
    "# Fit the PCA transformation to the scaled data\n",
    "ski_resort_pca = pca.fit_transform(ski_resort_data_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28322d1b-ebb2-4a4b-baa3-516fb6a647ed",
   "metadata": {},
   "source": [
    "Draw Scree Plot:\n",
    "The scree plot is used to visualize the amount of variance explained by each principal component. We can use the matplotlib library to draw a scree plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ba4b538-6b8e-476a-b72a-9249c9177a64",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PCA' object has no attribute 'explained_variance_ratio_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Calculate the percentage of variance explained by each component\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m variance_ratio \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplained_variance_ratio_\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create a bar chart of the explained variance\u001b[39;00m\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m), variance_ratio)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PCA' object has no attribute 'explained_variance_ratio_'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the percentage of variance explained by each component\n",
    "variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Create a bar chart of the explained variance\n",
    "plt.bar(range(1, 8), variance_ratio)\n",
    "\n",
    "# Add labels and a title\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d45f7-f281-4fa1-a5a0-397c88c03f07",
   "metadata": {},
   "source": [
    "The scree plot should show a sharp drop-off in variance explained after the first few components, with the majority of the variance explained by the first two components.\n",
    "\n",
    "Find the Optimum Number of Components:\n",
    "The optimal number of components can be determined by examining the scree plot and selecting the number of components that explain a significant proportion of the variance. In this case, we can see that the first two components explain the majority of the variance, so we will select these as the optimum number of components.\n",
    "\n",
    "Draw Biplot:\n",
    "The biplot is used to visualize the relationship between the principal components and the original features. We can use the matplotlib library to draw a biplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0a0520c-247b-4440-a111-4d3faf31a4b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (3738930164.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[22], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    plt.arrow(0, 0\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# Create a scatter plot of the first two principal components\n",
    "plt.scatter(ski_resort_pca[:, 0], ski_resort_pca[:, 1])\n",
    "\n",
    "# Add labels for each point\n",
    "for i, state in enumerate(ski_resort_data.index):\n",
    "    plt.annotate(state, (ski_resort_pca[i, 0], ski_resort_pca[i, 1]))\n",
    "\n",
    "# Add axis labels and a title\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Biplot')\n",
    "\n",
    "# Add vectors showing the contribution of each feature to the principal components\n",
    "for i, feature in enumerate(features):\n",
    "    plt.arrow(0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f82221-97ac-4817-8bdb-e1cb85471616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dataframe with the original features and their contributions to the principal components\n",
    "components_df = pd.DataFrame(pca.components_, columns=cols_to_scale)\n",
    "\n",
    "# Create a biplot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the scaled data points\n",
    "ax.scatter(scaled_data[:,0], scaled_data[:,1], alpha=0.5)\n",
    "\n",
    "# Add arrows for the original features\n",
    "for i, feature in enumerate(cols_to_scale):\n",
    "    ax.arrow(0, 0, pca.components_[0,i], pca.components_[1,i], head_width=0.1, head_length=0.1, color='red')\n",
    "    ax.text(pca.components_[0,i]*1.15, pca.components_[1,i]*1.15, feature, color='black', ha='center', va='center')\n",
    "\n",
    "# Add axis labels and title\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('Biplot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cecf97-9c52-4b8a-bebe-11a88c10e41f",
   "metadata": {},
   "source": [
    "Calculate Loadings:\n",
    "The loadings show the correlation between the original features and the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed17be58-cc9c-4b99-b36c-dc8a5f37a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the loadings matrix\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "# Create a dataframe with the loadings\n",
    "loadings_df = pd.DataFrame(loadings, columns=[f\"PC{i}\" for i in range(1, len(cols_to_scale)+1)], index=cols_to_scale)\n",
    "\n",
    "print(loadings_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e88872-ac2f-4045-813e-4ee9c149f105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
